{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN(Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://github.com/WegraLee/deep-learning-from-scratch<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Learning Process\n",
    ">1. Set node strcture(multiple layers), activation function(lelu, softmax,..) and loss function(mse,mae, cross entropy,...)\n",
    "2. Minimize loss function using gradient descent method<br>\n",
    "*There are many methods for minimizing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0442555   0.24983175 -0.29408725]\n",
      " [ 0.06638324  0.37474763 -0.44113087]]\n"
     ]
    }
   ],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([[1,2,3],[2,3,4]]).reshape(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2],[2,3],[3,4]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "w = np.random.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification \n",
    "def softmax(arr):\n",
    "    arr = arr - np.max(arr) \n",
    "    return(np.exp(arr)/sum(np.exp(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.dot(x,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sample\n",
    "#w=2, b=5\n",
    "np.random.seed(1000)\n",
    "X = np.random.normal(2,2,100)\n",
    "y = 2*X + 5 + np.random.normal(0,1,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe387b93748>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def mse(y,yPred):\n",
    "    value = (sum((y-yPred)**2))/len(y)\n",
    "    return(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.00233156025031"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict y to zero\n",
    "yPred = 0\n",
    "mse(y,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f,w,b,h=1e-4):\n",
    "    \n",
    "    wGrad = (f(y,X*(w+h)+b)-f(y,X*w+b))/h\n",
    "    bGrad = (f(y,X*w+b+h)-f(y,X*w+b))/h\n",
    "    return(wGrad,bGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model learning\n",
    "lamb = 1e-3 #step\n",
    "w,b = 1,1 #init value\n",
    "for i in range(1000): #iter is 1000\n",
    "    grad = gradient(mse,w,b)\n",
    "    w = w - lamb*grad[0]\n",
    "    b = b - lamb*grad[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3620967110925286 3.444689278958152\n"
     ]
    }
   ],
   "source": [
    "#similar to (w,b)=(2,5)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = X*w+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.689499527475509"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9327429605692269"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mse of answer value of w,b \n",
    "mse(y,X*2+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input node : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xw+b\n",
    "#w = (1,5,-3), b=5\n",
    "np.random.seed(10)\n",
    "x1 = np.random.normal(10,10,100)\n",
    "x2 = np.random.normal(5,10,100)\n",
    "x3 = np.random.normal(-5,10,100)\n",
    "\n",
    "y = x1+5*x2-3*x3+5+np.random.normal(0,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b=np.ones_like(x1)\n",
    "X = pd.DataFrame([x1,x2,x3]).T\n",
    "X.columns = ['x1','x2','x3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize init parameters\n",
    "np.random.seed(100)\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = np.dot(X,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54340494 0.27836939 0.42451759]\n",
      "[0.84477613]\n"
     ]
    }
   ],
   "source": [
    "#before learning\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5843.930822209296"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mse before learning \n",
    "mse(y,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_1d(f,w,b,h=1e-4):\n",
    "    yPred = np.dot(X,w)+b\n",
    "    wGrad = np.zeros_like(w)\n",
    "    bGrad = 0 #temp\n",
    "    for idx,i in enumerate(w):\n",
    "        w_ = w.copy()\n",
    "        b_ = b.copy()\n",
    "        \n",
    "        w_[idx] = w_[idx]+h\n",
    "       \n",
    "        yPred_ = np.dot(X,w_)+b\n",
    "        \n",
    "        wGrad[idx] = (f(y,yPred_)-f(y,yPred))/h\n",
    "    \n",
    "    b_ = b.copy()\n",
    "    b_ = b+h\n",
    "    yPred = np.dot(X,w)+b_\n",
    "    bGrad = (f(y,yPred_)-f(y,yPred))/h\n",
    "    \n",
    "    return(wGrad,bGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 1e-4\n",
    "for i in range(1000):\n",
    "    grad = gradient_1d(mse,w,b)\n",
    "    w = w - lamb*grad[0]\n",
    "    b = b - lamb*grad[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.33862193  5.16880288 -3.18721487]\n",
      "[-3.54718563]\n"
     ]
    }
   ],
   "source": [
    "#close to answer (w=(1,5,-3), b=5)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = np.dot(X,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.76356067093869"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.67332046856018"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#true w,b\n",
    "w=[1,5,-3]\n",
    "b=5\n",
    "mse(y,np.dot(X,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 layer\n",
    "3 - 4 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xw+b\n",
    "np.random.seed(10)\n",
    "x1 = np.random.normal(10,10,100)\n",
    "x2 = np.random.normal(5,10,100)\n",
    "x3 = np.random.normal(-5,10,100)\n",
    "\n",
    "y = x1+5*x2-3*x3+5+np.random.normal(0,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame([x1,x2,x3]).T\n",
    "X.columns = ['x1','x2','x3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1001)\n",
    "w1 = np.random.rand(3,4)\n",
    "b1 = np.random.rand(1)\n",
    "w2 = np.random.rand(4,1)\n",
    "b2 = np.random.rand(1)\n",
    "W = [w1,w2]\n",
    "B = [b1,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W,B,X):\n",
    "        outputTemp = np.dot(X,W[0])+B[0].T\n",
    "        for i in range(1,len(W)):\n",
    "            outputTemp = np.dot(outputTemp,W[i])+B[i]\n",
    "        return(outputTemp.reshape(len(X),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f,W,B,nth,h=1e-6):\n",
    "#     wGrad = np.zeros_like(W[nth])\n",
    "#     bGrad = np.zeros_like(B[nth])\n",
    "    \n",
    "    wGrad = []\n",
    "    bGrad = []\n",
    "    for i in range(W[nth].shape[0]):\n",
    "        \n",
    "        for j in range(W[nth].shape[1]):\n",
    "\n",
    "            W_ = W.copy()\n",
    "            \n",
    "            Wh = W[nth].copy()\n",
    "            \n",
    "            Wh[i,j] = W[nth][i,j]+h\n",
    "            W_[nth] = Wh\n",
    "            wGrad.append(( f(y,predict(W_,B,X)) - f(y,predict(W,B,X)) )/h)\n",
    "    \n",
    "#    wGrad = np.array(wGrad).reshape(W[nth].shape)\n",
    "    for i in range(len(B[nth])):\n",
    "        B_ = B.copy()\n",
    "        \n",
    "        Bh = B[nth].copy()\n",
    "        Bh[i] = B[nth][i] + h\n",
    "        \n",
    "        B_[nth] = Bh\n",
    "                  \n",
    "        bGrad.append( (f(y,predict(W,B_,X))-f(y,predict(W,B,X)))/h)\n",
    "   \n",
    "    wGrad = np.array(wGrad).reshape(W[nth].shape)\n",
    "    bGrad = np.array(bGrad).reshape(B[nth].shape)\n",
    "    \n",
    "    return((wGrad,bGrad))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w1(first layer) is 3x4(the number of input node is 3), w2(second layer) is 4x1\n",
    "#b1 is 4x1 and b2 is 1x1\n",
    "np.random.seed(1001)\n",
    "w1 = np.random.rand(3,4)\n",
    "b1 = np.random.rand(4,1)\n",
    "w2 = np.random.rand(4,1)\n",
    "b2 = np.random.rand(1)\n",
    "W = [w1,w2]\n",
    "B = [b1,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more slower to learning model \n",
    "lam = 1e-5\n",
    "for i in range(1000):\n",
    "    for n in range(2):\n",
    "        W[n] = W[n] - lam* gradient(mse,W,B,n)[0]\n",
    "        B[n] = B[n] - lam* gradient(mse,W,B,n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.41585012,  0.3432036 ,  0.2611423 ,  0.33518046],\n",
       "        [ 0.64045933,  0.37399317,  1.8498724 ,  0.67635428],\n",
       "        [-0.14872007,  0.38738077, -1.10712764, -0.61274598]]),\n",
       " array([[ 0.63857257],\n",
       "        [-0.00923913],\n",
       "        [ 2.19999628],\n",
       "        [ 0.86159259]])]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.53546686],\n",
       "        [0.93224894],\n",
       "        [0.23020233],\n",
       "        [0.44838607]]), array([0.78932797])]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.052057552325401"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#less than 1d ann and answer of w,b\n",
    "mse(y,predict(W,B,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used rough differential for gradient method. So it took much time to learn parameters.<br>\n",
    "If you use 'backpropagation' for gradient method, you'll get better result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
