{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN(Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://github.com/WegraLee/deep-learning-from-scratch<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Learning Process\n",
    ">1. Set node strcture(multiple layers), activation function(lelu, softmax,..) and loss function(mse,mae, cross entropy,...)\n",
    "2. Minimize loss function using gradient descent method<br>\n",
    "*There are many methods for minimizing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(arr):\n",
    "    arr = arr - np.max(arr)  # prevent overflow\n",
    "    return(np.exp(arr)/sum(np.exp(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(arr):\n",
    "    return 1/(1+np.exp(-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(arr):\n",
    "    return (abs(arr) + arr) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def mse(y,yPred):\n",
    "    value = (sum((y-yPred)**2))/len(y)\n",
    "    return(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of Input node = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize $w, b$ in $y'=wx+b$ subject to minimize $(y-y')^2$<br>\n",
    "($x,y,w,b$ is vector)<br>\n",
    "Use gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling\n",
    "#w=2, b=5\n",
    "np.random.seed(1000)\n",
    "X = np.random.normal(2,2,100)\n",
    "y = 2*X + 5 + np.random.normal(0,1,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=X,y=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f,w,b,h=1e-4):\n",
    "    \n",
    "    wGrad = (f(y,X*(w+h)+b)-f(y,X*w+b))/h\n",
    "    bGrad = (f(y,X*w+b+h)-f(y,X*w+b))/h\n",
    "    return(wGrad,bGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model learning\n",
    "lamb = 1e-3 #step\n",
    "w,b = 1,1 #init value\n",
    "for i in range(1000): #iter is 1000\n",
    "    grad = gradient(mse,w,b)\n",
    "    w = w - lamb*grad[0]\n",
    "    b = b - lamb*grad[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3620967110925286 3.444689278958152\n"
     ]
    }
   ],
   "source": [
    "#similar to (w,b)=(2,5)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.00233156025031"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict y to zero\n",
    "yPred = 0\n",
    "mse(y,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = X*w+b  # 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.689499527475509"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y,yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9327429605692269"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mse of answer value of w,b \n",
    "mse(y,X*2+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Method(using difference approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of Input node = 2 or more\n",
    "Update W, b using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSet(xParams:list, weights:list, bias:int, n:int,seed=2021):\n",
    "    \"\"\"\n",
    "    Make random data set X, y\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X = []\n",
    "    for p in xParams:\n",
    "        X.append(np.random.normal(p[0],p[1],n))\n",
    "    \n",
    "    for x,w in zip(X,weights):\n",
    "        try:\n",
    "            y = y+x*w\n",
    "        except NameError:\n",
    "            y = x*w\n",
    "    \n",
    "    xIdx = ['x'+str(i) for i in range(1,len(X)+1)]\n",
    "    X = pd.DataFrame(X)\n",
    "    X.index = xIdx\n",
    "    y += bias  # bias\n",
    "    y = y+np.random.normal(0,1,n)\n",
    "    \n",
    "    return(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 ~ N(10,10), x2 ~ N(5,10), x3 ~ N(-5,10)\n",
    "# y=Xw+b+e = x1+5x2-3x3+5+e\n",
    "xParams = [(10,10),(5,10),(-5,10)]\n",
    "weights = [1,5,-3]\n",
    "bias = 5\n",
    "X,y = dataSet(xParams,yParams,cParam,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>24.886091</td>\n",
       "      <td>16.760109</td>\n",
       "      <td>5.815486</td>\n",
       "      <td>1.934792</td>\n",
       "      <td>15.558758</td>\n",
       "      <td>2.944957</td>\n",
       "      <td>21.308583</td>\n",
       "      <td>16.450018</td>\n",
       "      <td>11.064137</td>\n",
       "      <td>14.221548</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.300329</td>\n",
       "      <td>22.610334</td>\n",
       "      <td>3.518744</td>\n",
       "      <td>32.453000</td>\n",
       "      <td>-0.123939</td>\n",
       "      <td>8.740613</td>\n",
       "      <td>5.675836</td>\n",
       "      <td>14.259372</td>\n",
       "      <td>17.566360</td>\n",
       "      <td>0.051459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>-2.405743</td>\n",
       "      <td>9.582332</td>\n",
       "      <td>17.572469</td>\n",
       "      <td>0.582959</td>\n",
       "      <td>10.413351</td>\n",
       "      <td>10.672784</td>\n",
       "      <td>0.646182</td>\n",
       "      <td>12.668124</td>\n",
       "      <td>9.808486</td>\n",
       "      <td>-3.812819</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.710303</td>\n",
       "      <td>8.534274</td>\n",
       "      <td>-3.277175</td>\n",
       "      <td>-9.965038</td>\n",
       "      <td>-10.176950</td>\n",
       "      <td>-5.379984</td>\n",
       "      <td>11.625670</td>\n",
       "      <td>-1.242281</td>\n",
       "      <td>-1.437624</td>\n",
       "      <td>-1.862540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>-0.587780</td>\n",
       "      <td>3.558804</td>\n",
       "      <td>-3.522833</td>\n",
       "      <td>-20.946331</td>\n",
       "      <td>21.368942</td>\n",
       "      <td>-12.165277</td>\n",
       "      <td>-8.879300</td>\n",
       "      <td>-29.050744</td>\n",
       "      <td>-0.584975</td>\n",
       "      <td>-0.695476</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.863222</td>\n",
       "      <td>10.255530</td>\n",
       "      <td>7.074615</td>\n",
       "      <td>21.691232</td>\n",
       "      <td>-3.889880</td>\n",
       "      <td>-16.265506</td>\n",
       "      <td>-6.203488</td>\n",
       "      <td>-17.265169</td>\n",
       "      <td>-12.226950</td>\n",
       "      <td>-11.190263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5   \\\n",
       "x1  24.886091  16.760109   5.815486   1.934792  15.558758   2.944957   \n",
       "x2  -2.405743   9.582332  17.572469   0.582959  10.413351  10.672784   \n",
       "x3  -0.587780   3.558804  -3.522833 -20.946331  21.368942 -12.165277   \n",
       "\n",
       "           6          7          8          9   ...         90         91  \\\n",
       "x1  21.308583  16.450018  11.064137  14.221548  ...  -3.300329  22.610334   \n",
       "x2   0.646182  12.668124   9.808486  -3.812819  ...  -5.710303   8.534274   \n",
       "x3  -8.879300 -29.050744  -0.584975  -0.695476  ... -18.863222  10.255530   \n",
       "\n",
       "          92         93         94         95         96         97  \\\n",
       "x1  3.518744  32.453000  -0.123939   8.740613   5.675836  14.259372   \n",
       "x2 -3.277175  -9.965038 -10.176950  -5.379984  11.625670  -1.242281   \n",
       "x3  7.074615  21.691232  -3.889880 -16.265506  -6.203488 -17.265169   \n",
       "\n",
       "           98         99  \n",
       "x1  17.566360   0.051459  \n",
       "x2  -1.437624  -1.862540  \n",
       "x3 -12.226950 -11.190263  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize init parameters\n",
    "np.random.seed(100)\n",
    "w = np.random.rand(3)\n",
    "b = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, -3]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = np.dot(w, X)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w :  [0.54340494 0.27836939 0.42451759]\n",
      "b :  [0.84477613]\n"
     ]
    }
   ],
   "source": [
    "# before learning\n",
    "print('w : ',w)\n",
    "print('b : ',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse :  5962.547891101834\n"
     ]
    }
   ],
   "source": [
    "# mse before learning \n",
    "print('mse : ', mse(y,yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_1d(f,w,b,h=1e-4):\n",
    "    \"\"\"\n",
    "    gradient descent(1d layer)\n",
    "    \"\"\"\n",
    "    yPred = np.dot(w, X)+b\n",
    "    wGrad = np.zeros_like(w)\n",
    "    bGrad = 0 #temp\n",
    "    for idx,i in enumerate(w):\n",
    "        w_ = w.copy()\n",
    "        b_ = b.copy()\n",
    "        \n",
    "        w_[idx] = w_[idx]+h\n",
    "        yPred_ = np.dot(w_,X)+b\n",
    "        \n",
    "        wGrad[idx] = (f(y,yPred_)-f(y,yPred))/h\n",
    "    \n",
    "    b_ = b.copy()\n",
    "    b_ = b+h\n",
    "    yPred = np.dot(w, X)+b_\n",
    "    bGrad = (f(y,yPred_)-f(y,yPred))/h\n",
    "    \n",
    "    return(wGrad,bGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 1e-4\n",
    "for i in range(1000):\n",
    "    grad = gradient_1d(mse,w,b)\n",
    "    w = w - lamb*grad[0]\n",
    "    b = b - lamb*grad[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.35046602  5.1601539  -3.19402507]\n",
      "[-3.50816205]\n"
     ]
    }
   ],
   "source": [
    "# close to answer (w=(1,5,-3), b=5)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = np.dot(w, X)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.083288866952618"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse of true w,b\n",
    "w=[1,5,-3]\n",
    "b=5\n",
    "mse(y,np.dot(w, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.105152611135978"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y,yPred)  # better mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of layer = 2 + use activation function(relu) for first layer\n",
    "input_layer = 3<br>\n",
    "middle layer = 4<br>\n",
    "output layer = 1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 ~ N(10,10), x2 ~ N(5,10), x3 ~ N(-5,10)\n",
    "# y=Xw+b+e = x1+5x2-3x3+5+e\n",
    "xParams = [(10,10),(5,10),(-5,10)]\n",
    "weights = [1,5,-3]\n",
    "bias = 5\n",
    "X,y = dataSet(xParams,yParams,cParam,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init parameters for optimize(W, B)\n",
    "# w1(first layer) is 3x4(the number of input node is 3), w2(second layer) is 4x1\n",
    "# b1 is 4x1 and b2 is 1x1\n",
    "np.random.seed(1001)\n",
    "w1 = np.random.rand(4,3)\n",
    "b1 = np.random.rand(4,1)\n",
    "w2 = np.random.rand(1,4)\n",
    "b2 = np.random.rand(1)\n",
    "W = [w1,w2]\n",
    "B = [b1,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W,B,X):\n",
    "        outputTemp = relu(np.dot(W[0],X)+B[0])\n",
    "        for i in range(1,len(W)-1):\n",
    "            outputTemp = np.dot(W[i], outputTemp)+B[i]\n",
    "            \n",
    "        outputTemp = np.dot(W[-1], outputTemp)+B[-1]\n",
    "        return(outputTemp.reshape(X.shape[-1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f,W,B,nth,h=1e-6):\n",
    "    wGrad = []\n",
    "    bGrad = []\n",
    "    for i in range(W[nth].shape[0]):\n",
    "        \n",
    "        for j in range(W[nth].shape[1]):\n",
    "\n",
    "            W_ = W.copy()\n",
    "            \n",
    "            Wh = W[nth].copy()\n",
    "            \n",
    "            Wh[i,j] = W[nth][i,j]+h\n",
    "            W_[nth] = Wh\n",
    "            wGrad.append(( f(y,predict(W_,B,X)) - f(y,predict(W,B,X)) )/h)\n",
    "    \n",
    "    for i in range(len(B[nth])):\n",
    "        B_ = B.copy()\n",
    "        \n",
    "        Bh = B[nth].copy()\n",
    "        Bh[i] = B[nth][i] + h\n",
    "        \n",
    "        B_[nth] = Bh\n",
    "                  \n",
    "        bGrad.append( (f(y,predict(W,B_,X))-f(y,predict(W,B,X)))/h)\n",
    "   \n",
    "    wGrad = np.array(wGrad).reshape(W[nth].shape)\n",
    "    bGrad = np.array(bGrad).reshape(B[nth].shape)\n",
    "    \n",
    "    return((wGrad,bGrad))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 63.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# more slower to learning model \n",
    "lam = 1e-4\n",
    "for i in tqdm(range(2000)):\n",
    "    for n in range(2):\n",
    "        W[n] = W[n] - lam* gradient(mse,W,B,n)[0]\n",
    "        B[n] = B[n] - lam* gradient(mse,W,B,n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.003154928664543"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar to 1d\n",
    "mse(y,predict(W,B,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Method(using backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta W^{(l)}=\\frac{1}{N}\\Delta^{(l)}Z^{(l-1)T}$, $(l)$means layer order. Weight of input layer is $W^{(1)}$<br>\n",
    "$\\delta b^{(l)}=\\frac{1}{N}\\Delta^{(l)}1_N$<br>\n",
    "$\\Delta^{(l)}=f^{(l)'}(U^{(l)}) \\bigodot (W^{(l+1)T}\\Delta^{(l+1)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 ~ N(10,10), x2 ~ N(5,10), x3 ~ N(-5,10)\n",
    "# y=Xw+b+e = x1+5x2-3x3+5+e\n",
    "xParams = [(10,10),(5,10),(-5,10)]\n",
    "weights = [1,5,-3]\n",
    "bias = 5\n",
    "X,y = dataSet(xParams,yParams,cParam,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init parameters for optimize(W, B)\n",
    "# w1(first layer) is 3x4(the number of input node is 3), w2(second layer) is 4x1\n",
    "# b1 is 4x1 and b2 is 1x1\n",
    "np.random.seed(1001)\n",
    "w1 = np.random.rand(4,3)\n",
    "b1 = np.random.rand(4,1)\n",
    "w2 = np.random.rand(1,4)\n",
    "b2 = np.random.rand(1)\n",
    "W = [w1,w2]\n",
    "B = [b1,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,3,-2,-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_prime(arr):\n",
    "    return np.array(arr>=0, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, B, X):\n",
    "    U = [np.dot(W[0], X)+B[0]]\n",
    "    Z = [relu(U[0])]\n",
    "    for i in range(1, len(W)-1):\n",
    "        U.append(np.dot(W[i], Z[-1])+B[i])\n",
    "        Z.append(relu(U[-1]))\n",
    "    U.append(np.dot(W[-1], Z[-1])+B[-1])\n",
    "    Z.append(U[-1])  # output layer don't use activate function\n",
    "    return U, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(W, B, U, Z, X, y, lam):\n",
    "    D = []\n",
    "    n = X.shape[-1]\n",
    "    pred = Z[-1]\n",
    "    for i in reversed(range(len(W))):  # length of layer\n",
    "        if i == len(W)-1:\n",
    "            delta = pred - y\n",
    "        else:\n",
    "            delta = relu_prime(U[i]) * (np.dot(W[i+1].T, delta))\n",
    "        if i == 0:\n",
    "            z = X  # input data\n",
    "        else:\n",
    "            z = Z[i-1]\n",
    "        dw = np.dot(delta, z.T) / n\n",
    "        db = np.sum(delta) / n\n",
    "        W[i] = W[i] - dw*lam\n",
    "        B[i] = B[i] - db*lam\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    U, Z = forward(W, B, X)\n",
    "    backward(W, B, U, Z, X, y, 10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.535984520817987"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more faster and better result(mse)\n",
    "mse(y, forward(W, B, X)[1][-1].reshape(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
